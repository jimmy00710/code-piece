{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References - <br> \n",
    "> https://stackoverflow.com/questions/50747947/embedding-in-pytorch <br>\n",
    "> https://stats.stackexchange.com/questions/270546/how-does-keras-embedding-layer-work\n",
    "> https://stackoverflow.com/questions/49466894/how-to-correctly-give-inputs-to-embedding-lstm-and-linear-layers-in-pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Embedding Layer <br>  \n",
    "* Embedding layer is a vector representation of all the words or sentences or characters. It act as a lookup table. By lookup table I mean assuming that each index/number represent a word so each number will have a vector in the lookup table. <br>\n",
    "* So what is the argument of embedding layer, what is the input of embedding layer what it gives us as an output  ? <br>\n",
    "> Input arguments - vocab size ( number of unique words if we are considering word embedding or unqiue sentence if we are considering sentence embedding), vector size ( or the embedding size which it will give us as an output) <br>\n",
    "> Input - batch size or list ( where list act as the batch uske andar hamare sentences hai because of that sequence length hai next argument or dimension) , sequence length <br>\n",
    "> Output - Batch size,sequence length, vector size <br> \n",
    "\n",
    "Lets understand it with an example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex1 = 'This tutorial is amazing'\n",
    "ex2 = 'I know it is'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "list_of_unique_words = <b>[This,tutorial, is amazing, I know, it] </b><br>\n",
    "\n",
    "vocab_size = length of this list = <b> length(list_of_unique_words)  </b><br>\n",
    "\n",
    "vocab_size = 7 <br> \n",
    "\n",
    "vector_size = 3 (it is given by user) \n",
    "\n",
    "batch_size = length of total number of sentences we are passing, here we have 2 sentences. <br> \n",
    "batch_size = 2 <br> \n",
    "\n",
    "sequence_length = length of biggest sentence. In our example it is 4. <br>\n",
    "sequence_length = 4 <br>\n",
    "\n",
    "So from above things our output should have following dimension -  <br> \n",
    "Output - 2, 4,3 <br> \n",
    "\n",
    "Before starting we have to convert our each word into a number because computer can't understand strings. <br> \n",
    "So for simplicity lets assign them numbers in the following manner - <br> \n",
    "\n",
    "This - 0 <br>\n",
    "tutorial - 1 <br>\n",
    "is - 2 <br>\n",
    "amazing - 3 <br> \n",
    "I - 4 <br> \n",
    "know - 5 <br> \n",
    "it - 6 <br> \n",
    "\n",
    "our new examples will look like this - \n",
    "\n",
    "ex1 = [0,1,2,3] <br> \n",
    "ex2 = [4,5,6,2] <br> \n",
    "\n",
    "The batch will be  <br>  \n",
    "b = [ex1,ex2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In Summary we can say Embedding takes following input and returns following output   <br>\n",
    "> ####  Params --> (vocab_size (unique list of words), embedding vector)\n",
    "> ####  Input --> (batch_of_sentences or batch and max_sequence length from matrix prespective) \n",
    "> ####  Output --> (batch_size,max_sequence_length( maximum length of input), embedding vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.1544, -0.1875,  1.0524],\n",
       "         [ 1.2851, -0.1237, -1.4782],\n",
       "         [-1.7223,  2.0916, -1.4331],\n",
       "         [ 0.2040, -0.5076, -0.1357]],\n",
       "\n",
       "        [[ 0.2340,  0.0438, -1.1761],\n",
       "         [ 0.3264, -0.1366,  0.7144],\n",
       "         [-0.1838, -0.2113, -0.7216],\n",
       "         [-1.7223,  2.0916, -1.4331]]], grad_fn=<EmbeddingBackward>)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b  = torch.LongTensor([[0,1,2,3],[4,5,6,2]])\n",
    "embedding = nn.Embedding(7,3)\n",
    "output = embedding(b)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding vector lookup looks like this - <br>\n",
    "\n",
    "This - 0 - -0.1544, -0.1875,  1.0524 <br> \n",
    "is - 1 - 1.2851, -0.1237, -1.4782 <br>  \n",
    "\n",
    "So each index represent each value and there embedding too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Way to display image in markdown\n",
    "#The pictures are from colah blog.\n",
    "#![RNN Layer](https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png=1550)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN cell looks like this \n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-rolled.png\" width=\"100\" height=\"100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Layer Looks like this :\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/RNN-unrolled.png\" width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see from above diagram RNN cell takes 2 inputs and returns 2 outputs.  <br>\n",
    "> *  The input are input sequence and hidden vector. <br>\n",
    "> *  The output is hidden vector and output cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.8988e-01,  6.1117e-01, -1.2516e+00, -2.6599e-01,  1.6506e+00,\n",
       "           1.4007e+00,  1.5334e-01,  3.2586e-01, -9.3827e-01, -1.8486e+00],\n",
       "         [-3.3065e-01,  5.2652e-01,  1.2147e+00, -1.1235e+00,  6.1428e-01,\n",
       "          -2.7109e-01,  9.4458e-01, -6.1269e-06, -9.6571e-01,  5.9961e-01],\n",
       "         [ 4.6273e-01,  3.8383e-01,  9.8650e-01,  1.4029e-01, -1.2388e+00,\n",
       "           2.2979e+00,  3.9534e-01,  4.7278e-01,  1.1889e+00, -4.1915e-01]],\n",
       "\n",
       "        [[-8.8662e-01,  1.0195e+00,  5.7871e-01,  1.0204e+00, -1.5576e-01,\n",
       "           4.2497e-01,  7.4028e-01,  8.4036e-01,  3.2050e-01,  7.3251e-01],\n",
       "         [-1.1737e-01, -1.2300e+00,  7.2090e-01, -2.7959e-01,  3.0336e-01,\n",
       "          -1.1116e+00,  1.2790e-01,  1.1594e+00, -2.1750e-01, -2.2292e+00],\n",
       "         [ 1.8969e+00,  1.8047e+00,  1.3417e-01, -1.3703e+00, -7.1033e-01,\n",
       "           1.2285e+00, -9.6645e-01,  1.1403e+00, -1.0947e+00, -1.2524e+00]],\n",
       "\n",
       "        [[ 5.2239e-01, -1.0371e-01,  1.3721e+00,  4.1747e-01, -1.7231e+00,\n",
       "          -1.4029e-01, -2.2437e-01,  1.9434e-01,  9.3819e-01, -2.5154e+00],\n",
       "         [ 2.6324e-01, -4.6484e-02, -4.0144e-01, -1.2957e+00,  3.3543e-01,\n",
       "           8.8802e-01, -2.2098e+00, -1.1386e+00,  1.7210e-01, -4.0574e-01],\n",
       "         [-6.4068e-01, -6.3719e-01,  4.6825e-01, -1.4748e+00,  1.0660e+00,\n",
       "          -4.0868e-01, -1.1132e+00, -6.6112e-01,  1.3566e+00,  6.8189e-01]],\n",
       "\n",
       "        [[-1.1021e+00,  1.6227e-02, -1.5627e-01,  2.9326e-01, -1.9732e-02,\n",
       "          -4.0146e-01,  5.9948e-01, -7.3197e-01,  1.1840e+00, -3.5317e-01],\n",
       "         [-7.2003e-02, -8.1620e-01,  2.2678e-01, -8.8002e-01, -2.7676e-01,\n",
       "           6.3606e-01, -4.6113e-01,  8.1666e-01,  1.4856e-01,  7.9950e-01],\n",
       "         [-5.1671e-01, -3.6722e-01,  7.0399e-01,  8.7021e-01,  4.0920e-01,\n",
       "           1.4581e+00, -5.8031e-01, -6.5439e-01, -1.3933e+00,  1.0884e+00]],\n",
       "\n",
       "        [[-2.0435e+00, -2.8340e-01, -1.5594e+00,  1.6238e+00, -3.3827e-01,\n",
       "          -2.5559e-01, -5.0025e-01,  9.3488e-01,  2.2985e+00, -2.1432e+00],\n",
       "         [-3.3844e-01, -1.7414e+00,  9.9375e-02,  1.7788e+00, -1.0131e+00,\n",
       "          -1.0888e+00,  6.6928e-01, -1.9904e+00, -1.7850e+00, -5.6390e-01],\n",
       "         [-2.5156e-01,  5.2808e-01, -5.9012e-01,  7.5650e-01, -5.4508e-01,\n",
       "           1.8452e+00, -8.2588e-02, -1.0205e+00,  7.2065e-02,  1.4508e+00]]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.randn(5, 3, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch = 2, seq= 2, input= 2\n",
    "inputs = torch.randn(2,2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number_layers * direction = 1 layer * 1 direction , batch=  2 , vector = 3\n",
    "hidden = torch.randn(1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = torch.nn.RNN(2,3,1,batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs,hiddens = rnn(inputs,hidden)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3622,  0.3926,  0.6566],\n",
       "         [-0.3643,  0.4057,  0.2548]],\n",
       "\n",
       "        [[-0.4595,  0.5373,  0.2882],\n",
       "         [-0.3626,  0.3899,  0.1920]]], grad_fn=<TransposeBackward1>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 4, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3622,  0.3926,  0.6566]],\n",
       "\n",
       "         [[-0.3643,  0.4057,  0.2548]]],\n",
       "\n",
       "\n",
       "        [[[-0.4595,  0.5373,  0.2882]],\n",
       "\n",
       "         [[-0.3626,  0.3899,  0.1920]]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#batch,b\n",
    "#outputs.view(seq_len, batch, num_directions, hidden_size)\n",
    "outputs.view(2, 2, 1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3643,  0.4057,  0.2548],\n",
       "         [-0.3626,  0.3899,  0.1920]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.4343, -1.1487, -0.6000],\n",
       "         [ 1.4566, -0.7191,  1.2151]]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.0572,  0.4944],\n",
       "         [ 1.0124, -0.2122]],\n",
       "\n",
       "        [[-0.0909, -0.2828],\n",
       "         [ 0.1539,  0.0042]]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2, 3])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3643,  0.4057,  0.2548],\n",
       "          [-0.3626,  0.3899,  0.1920]]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#h_n.view(num_layers, num_directions, batch, hidden_size).\n",
    "hiddens.view(1,1,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = outputs.view(1,2,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.3622,  0.3926,  0.6566],\n",
       "          [-0.3643,  0.4057,  0.2548]],\n",
       "\n",
       "         [[-0.4595,  0.5373,  0.2882],\n",
       "          [-0.3626,  0.3899,  0.1920]]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3622,  0.3926,  0.6566],\n",
       "         [-0.3643,  0.4057,  0.2548]],\n",
       "\n",
       "        [[-0.4595,  0.5373,  0.2882],\n",
       "         [-0.3626,  0.3899,  0.1920]]], grad_fn=<ViewBackward>)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs.view(2,2,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.3643,  0.4057,  0.2548], grad_fn=<SelectBackward>)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.3643,  0.4057,  0.2548],\n",
       "         [-0.3626,  0.3899,  0.1920]]], grad_fn=<StackBackward>)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hiddens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hindi mein isko thoda lete hai. Kitne batch hai. Ek batch mein kitne words hai and un words ko agar vector space mein represent kre to kaise rehga.<br>\n",
    "\n",
    "So when I said batch size of 2 matlab 2 batch lene hai and sequence length of 2 matlab ek batch mein max sequence itni words rehengs and input_size or vector size mane vector space mein usko itne vector value represent krenge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i> So ye jo kr reh h vo bht simple hai ek misundertanding apan se ho gayi. So when I say batch matlab vo ek bar mein jayega. Once one batch has completed tab dusra batch jayega. So agar hum upar ke outputs dekhe to 2 batches hai. Phela batch aya. Ab maximum sequence length hai 2. So index 1 position ke value hidden value hiddens ne store kr liye and output ne sab kch store kr liya and ne </i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how pytorch stores vector in hidden and output layer. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://miro.medium.com/max/640/1*tUxl5-C-t3Qumt0cyVhm2g.png\" width=\"500\" height=\"200\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References on RNN \n",
    "> * https://towardsdatascience.com/pytorch-basics-how-to-train-your-neural-net-intro-to-rnn-cb6ebc594677 \n",
    "> * https://towardsdatascience.com/recurrent-neural-networks-d4642c9bc7ce\n",
    "> * https://datascience-enthusiast.com/DL/Building_a_Recurrent_Neural_Network-Step_by_Step_v1.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
