{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Approching a problem in  Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is about how to approach different ML problems. How to go about those problems and which algorithm  to use and how to write the code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So I have gone through one of the interview of Abhishek Thakur with Sayak and there he was asked how he learn about different things. There the GrandMaster said that he learn by applying different things on different problem set using this he learnt a lot of different things. This is what I also think that we should be doing --> instead of going directly for the SOTA we should start from the basic approach and build on that. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So today I am starting on Resume filtering solution or Resume classification solution. I can start from simple logistic regression , naive bayes, Decision Tree, and Random Forest. Once we have applied these algos we can check which one is working and how they are working and what are the hyperparameter I can change. We know that RNN/Transformer  are the go to method for the NLP problems now a days but to get a better understanding on why they work and why the traditional algorithm didn't work we can do it in this way. For most of the NLP based classification problem we can build a pipeline sort of thing where our code is preprocessing everything and after that with few clicks we are able to apply different algorithm. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in Data Science we perform a lot of experiment. It will be very good if our code is modular. So instead of writing the same code or pasting the same code again and again it will be much better if we import there modules. In this way we will not be copying anything and it will be efficient too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So these will be my basic algorithm which I will be implementing - \n",
    "> * Logistic Regression\n",
    "> * Naive Bayes \n",
    "> * Decision Tree\n",
    "> * RandomForest\n",
    "> * K-Nearest Neighbour\n",
    "> * SVM ( Soft Vector Machine)\n",
    "> * SGD (Stochastic Gradient Descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now before starting let us also think the preprocessing step or the EDA we can do for the NLP data -\n",
    "> * BagOfWords\n",
    "> * TfIdf\n",
    "> * Word2Vec\n",
    "> * SkipGram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing involves a lot of things in text data. Here is a link of different preprocessing steps in NLP. <br>\n",
    "\n",
    "<a href=\"https://www.kdnuggets.com/2019/04/text-preprocessing-nlp-machine-learning.html\"> Blog Link </a> <br> \n",
    "\n",
    "Methods discussed in blog are as follows - \n",
    "> * Lowercasing \n",
    "> * Stemming\n",
    "> * Lemmatization\n",
    "> * Stopword Removal\n",
    "> * Text Normalization\n",
    "> * Noise Removal\n",
    "> * Text Enrihment / Augmentations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us look how to do implement all of the preprocessing steps and create modular function which can be controlled by just few arguments. The whole purpose of this function will be to give user a flexibility to perform different sorts of natural language preprocessing easily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting I will like to point out the libraries which are really important for our preprocessing. I will add these libraries in the requirements text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * Spacy\n",
    "> * NLTK\n",
    "> * BeautifulSoup\n",
    "> * uniencode\n",
    "> * word2number\n",
    "> * contractions \n",
    "> * inflect "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> * <a href=\"https://www.kdnuggets.com/2018/03/text-data-preprocessing-walkthrough-python.html\"> KD Nuggests Data Preprocessing Walkthrough </a> \n",
    "> * <a href=\"https://towardsdatascience.com/nlp-text-preprocessing-a-practical-guide-and-template-d80874676e79\"> NLP text preprocessing </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'can', 'tell', 'would', 'shake', 'turn', 'talk', 'say']\n",
      "Sebastian Thrun PERSON\n",
      "Google ORG\n",
      "2007 DATE\n",
      "American NORP\n",
      "Thrun PERSON\n",
      "Recode LOC\n",
      "earlier this week DATE\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print(entity.text, entity.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacy."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
